{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2067ba8c-6c5b-436d-ad35-2b33cb75a307",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "71a428c6-c84b-465c-becc-9c5b5d069147",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ce221c9a-130f-4478-b518-fc6d9e50134e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c11c1e86-f36b-46c8-977a-e0f2622678ee",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Declare the path\n",
    "bronze_path = \"abfss://bronze@joecryptostorage01.dfs.core.windows.net/\"\n",
    "silver_path = \"abfss://silver@joecryptostorage01.dfs.core.windows.net/silver_crypto\"\n",
    "schema_path = \"abfss://schema@joecryptostorage01.dfs.core.windows.net/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "269cfb2f-b4db-4cb1-9dbb-d7cf83887995",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Auto Loader Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bbdb5ebc-8cc4-4575-80d0-b56b182638e2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_crypto = spark.readStream\\\n",
    "    .format(\"cloudFiles\")\\\n",
    "    .option(\"cloudFiles.format\", \"json\")\\\n",
    "    .option(\"cloudFiles.schemaEvolutionMode\", \"addNewColumns\")\\\n",
    "    .option(\"cloudFiles.schemaLocation\", schema_path)\\\n",
    "    .option(\"cloudFiles.schemaHints\", \n",
    "            \"\"\"\n",
    "            roi STRUCT<times:DOUBLE, currency:STRING, percentage:DOUBLE>,\n",
    "            ath STRING,\n",
    "            ath_change_percentage STRING,\n",
    "            ath_date STRING,\n",
    "            atl STRING,\n",
    "            atl_change_percentage STRING,\n",
    "            atl_date STRING,\n",
    "            circulating_supply STRING,\n",
    "            current_price STRING,\n",
    "            high_24h STRING,\n",
    "            id STRING,\n",
    "            last_updated STRING,\n",
    "            low_24h STRING,\n",
    "            market_cap STRING,\n",
    "            market_cap_change_24h STRING,\n",
    "            market_cap_change_percentage_24h STRING,\n",
    "            market_cap_rank STRING,\n",
    "            max_supply STRING,\n",
    "            name STRING,\n",
    "            price_change_24h STRING,\n",
    "            price_change_percentage_24h STRING,\n",
    "            symbol STRING,\n",
    "            total_supply STRING,\n",
    "            total_volume STRING\n",
    "            \"\"\") \\\n",
    "    .load(bronze_path)\\\n",
    "    .withColumn(\"ingested_time\", current_timestamp())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ff7bb290-3168-4dcd-bbc3-3bfd0e974451",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_crypto.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "080166d5-f378-48cf-bbd4-13eb43d21d8a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Transforming Data using Pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "89087212-d62c-45f7-bc9a-edd84e1d80c5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Creates a new column i.e roi_time by extracting the times field from the roi struct column.\n",
    "df_parsed = df_crypto\\\n",
    "  .withColumn(\"roi_time\", col(\"roi.times\"))\\\n",
    "  .withColumn(\"roi_currency\", col(\"roi.currency\"))\\\n",
    "  .withColumn(\"roi_percentage\", col(\"roi.percentage\"))\\\n",
    "  .drop('roi')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b1cea759-3d98-4014-b705-78aec5b070fc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_parsed.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "08775499-0cbb-475d-86be-4c079e3dbde5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_transformed3.display()\n",
    "# df_parsed.show(n=10, truncate=False) #-to display few rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0935ddc9-52eb-4ebb-98fe-ad32ba70dd2e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Replacing the Z or +00:00 with empty string and converting the last_updated column to timestamp\n",
    "from pyspark.sql.functions import regexp_replace, to_timestamp, to_date, when\n",
    "df_transformed1 = df_parsed \\\n",
    "    .withColumn(\"last_updated_clean\", regexp_replace(col(\"last_updated\"), \"(Z|\\\\+00:00)$\", \"\")) \\\n",
    "    .withColumn(\n",
    "        \"last_updated_ts\",\n",
    "        when(\n",
    "            col(\"last_updated_clean\").isNotNull(),\n",
    "            to_timestamp(col(\"last_updated_clean\"), \"yyyy-MM-dd'T'HH:mm:ss.SSS\")\n",
    "        )\n",
    "        .otherwise(current_timestamp())\n",
    "    )\\\n",
    "    .withColumn(\"date_partition\", to_date(col(\"last_updated_ts\"))) \\\n",
    "    .drop(\"last_updated\", \"last_updated_clean\")\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7d3d288b-5adf-4ef2-bcf7-a30da86dd746",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "df_transformed2 = df_transformed1 \\\n",
    "    .withColumn(\"atl_date_clean\", regexp_replace(col(\"atl_date\"), \"(Z|\\\\+00:00)$\", \"\")) \\\n",
    "    .withColumn(\n",
    "        \"atl_date_ts\",\n",
    "        when(\n",
    "            col(\"atl_date_clean\").isNotNull(),\n",
    "            to_timestamp(col(\"atl_date_clean\"), \"yyyy-MM-dd'T'HH:mm:ss.SSS\")\n",
    "        )\n",
    "        .otherwise(current_timestamp())\n",
    "    )\\\n",
    "    .withColumn(\"atl_date_partition\", to_date(col(\"atl_date_ts\"))) \\\n",
    "    .drop(\"atl_date\", \"atl_date_clean\")\n",
    "\n",
    "\n",
    "df_transformed3 = df_transformed2 \\\n",
    "    .withColumn(\"ath_date_clean\", regexp_replace(col(\"ath_date\"), \"(Z|\\\\+00:00)$\", \"\")) \\\n",
    "    .withColumn(\n",
    "        \"ath_date_ts\",\n",
    "        when(\n",
    "            col(\"ath_date_clean\").isNotNull(),\n",
    "            to_timestamp(col(\"ath_date_clean\"), \"yyyy-MM-dd'T'HH:mm:ss.SSS\")\n",
    "        )\n",
    "        .otherwise(current_timestamp())\n",
    "    )\\\n",
    "    .withColumn(\"ath_date_partition\", to_date(col(\"ath_date_ts\"))) \\\n",
    "    .drop(\"ath_date\", \"ath_date_clean\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ebc11173-cb50-4ac2-8967-4593835c9f96",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_transformed3.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ab349614-9273-4edd-86b9-df92e46d96c4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# convert the market_cap and total_volume columns to double data types.\n",
    "# create a new column named \"market_cap_billions\" that converts the market_cap column to billions.\n",
    "# create a calculated column named \"volume_to_market_cap_ratio\" that calculates the ratio of total_volume to market_cap.\n",
    "df_metrics = df_transformed3 \\\n",
    "    .withColumn(\"market_cap_double\", col(\"market_cap\").cast(\"double\")) \\\n",
    "    .withColumn(\"total_volume_double\", col(\"total_volume\").cast(\"double\")) \\\n",
    "    .withColumn(\"market_cap_billions\", \n",
    "        when(col(\"market_cap_double\").isNotNull(), col(\"market_cap_double\") / 1000000000.0)\n",
    "        .otherwise(0.0)\n",
    "    ) \\\n",
    "    .withColumn(\"volume_to_market_cap_ratio\",\n",
    "        when(\n",
    "            (col(\"total_volume_double\").isNotNull()) & \n",
    "            (col(\"market_cap_double\").isNotNull()) & \n",
    "            (col(\"market_cap_double\") > 0),\n",
    "            col(\"total_volume_double\") / col(\"market_cap_double\")\n",
    "        ).otherwise(0.0)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0ec09837-914d-4034-93ed-116d10532003",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_metrics.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6a8ebab3-033d-4cd0-9d65-801e777c7c1d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Starts with a DataFrame called df_metrics.\n",
    "#Applies a filter to remove rows where any of the specified columns have null values.\n",
    "#The result is stored in a new DataFrame called df_final.\n",
    "df_final = df_metrics \\\n",
    "    .filter(\n",
    "    col(\"symbol\").isNotNull() & \n",
    "    col(\"last_updated_ts\").isNotNull() &\n",
    "    col(\"date_partition\").isNotNull() &\n",
    "    col(\"market_cap_billions\").isNotNull() &\n",
    "    col('id').isNotNull()\n",
    "    )\\\n",
    "    .dropDuplicates(['id', 'last_updated_ts'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d28f57dc-cbed-45ab-9028-cafc9059a08c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_final.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0a20023f-c987-41f0-ac2a-2deea7082b3e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SHOW EXTERNAL LOCATIONS;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7dfa0f54-c9b6-4696-8f01-69e08bfc35f8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Writing to Silver Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dc971ded-14ac-4ce2-9728-250d88c00251",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Start writing df_final as a structured streaming output.\n",
    "#Use Delta Lake format for output (ACID transactions, versioning, schema evolution).\n",
    "#Append only new data to the destination (no updates or overwrites).\n",
    "#Define a checkpoint directory to store the stream’s state → needed for fault tolerance and exactly-once processing.\n",
    "#Process all available data in one go, then stop the stream. (A common pattern for micro-batch or scheduled jobs).\n",
    "#Start writing to the location specified by silver_path.\n",
    "cryptodataout = df_final.writeStream \\\n",
    "    .format(\"delta\")\\\n",
    "    .outputMode(\"append\")\\\n",
    "    .option(\"checkpointLocation\", \"abfss://silver@joecryptostorage01.dfs.core.windows.net/_checkpoints/crypto_market\")\\\n",
    "    .trigger(once=True)\\\n",
    "    .start(silver_path)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 8452237505465812,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "silver-layer 2025-06-18",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
